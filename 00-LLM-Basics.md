# 00 LLM Basics 


## LLM Basic Concept

| Concept | Explanation |
|---------|-------------|
| Cost per million input tokens | The model charges **$3** for processing every **1 million input tokens**. Input tokens are the tokens sent to the model as part of the request (e.g., user queries or prompts). |
| Cost per million output tokens | The model charges **$15** for every **1 million output tokens** generated in response to the input. Output tokens refer to the tokens generated by the model as part of the response (e.g., chatbot replies). |
| Token | Tokens are pieces of words. For example, "ChatGPT is great" might be split into 4 tokens: "Chat", "GPT", "is", "great". Tokens could be words, parts of words, or punctuation marks. |
| 200K token context window | The model has a **200,000-token context window**, meaning it can handle up to 200,000 tokens in memory for a single input/output session. This includes both the input tokens and output tokens, allowing the model to "remember" a long conversation or document. |


## The factors to consider when selecting a language model. 

| Factor | Description |
|--------|-------------|
| 1. Model Performance | - Quality of responses for specific use cases<br>- Specialization for domains (e.g., medical, legal, coding)<br>- Performance on standard benchmarks (e.g., SQuAD, GLUE, MMLU) |
| 2. Latency and Speed | - Response time for real-time applications<br>- Inference time and trade-offs with accuracy<br>- Scalability for handling large numbers of requests |
| 3. Model Size (Parameters) | - Smaller models: faster, cost-effective, but may lack depth<br>- Larger models: better results, but more resource-intensive |
| 4. Compute and Hardware Requirements | - GPU/CPU needs for different model sizes<br>- RAM and storage requirements<br>- Optimizations like quantization or distillation |
| 5. Fine-Tuning and Adaptability | - Capability for domain-specific fine-tuning<br>- Data requirements for customization<br>- Training budget considerations |
| 6. Tokenization and Language Support | - Multilingual support for global user bases<br>- Tokenization efficiency for various languages and domains |
| 7. Privacy and Data Security | - On-premises vs. cloud deployment options<br>- Compliance with regulations (e.g., GDPR, HIPAA)<br>- Data retention policies |
| 8. Integration and Ecosystem Support | - Availability of APIs and SDKs<br>- Access to pre-trained models<br>- Integration with existing cloud services |
| 9. Cost-Effectiveness | - API pricing for input and output tokens<br>- Compute costs for on-premises deployment<br>- Open-source vs. commercial model considerations |
| 10. Community and Support | - Strength of community resources and support<br>- Vendor support for commercial models<br>- Availability of pre-built tools and libraries |
| 11. Energy Efficiency and Sustainability | - Power consumption considerations<br>- Availability of energy-efficient models or initiatives |
| 12. Explainability and Interpretability | - Model transparency for critical applications<br>- Bias and fairness considerations |
| 13. Task-Specific Models | - Availability of pre-trained models for specific domains or tasks |
| 14. Longevity and Future-Proofing | - Frequency of model updates<br>- Provider's future roadmap and continued support |


## LLM Foundation Model | Inference parameters - 2 Types
### 1. Randomness and diversity 
- Temperature: controls randomness in word choice. Lower values lead to more predictable responses. 
- Top K limits word choices to the K most probable options. Lower values reduce unusual responses.
- Top P cuts off low probability word choices based on cumulative probability. It tightens overall response distribution.

### 2. Length
- Response length sets minimum and maximum token counts. It sets a hard limit on response size.
- Length penalty encourages more concise responses by penalizing longer ones. It sets a soft limit on size.
- Stop sequences include specific character combinations that signal the model to stop generating tokens when encountered. It is used for the early termination of responses.


## **five popular chat models** in tabular format:

| **Model**                   | **OpenAI GPT-4**                              | **Anthropic Claude 2**                        | **Google PaLM 2 (Bison)**                    | **Meta LLaMA 2**                             | **Mistral 7B**                               |
|-----------------------------|-----------------------------------------------|-----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|
| **Architecture**             | Transformer-based, autoregressive             | Transformer-based, safety and alignment-focused | Transformer-based, focuses on code, reasoning | Transformer-based, optimized for efficiency | Transformer-based, lightweight & highly efficient |
| **Parameter Size**           | 175B (GPT-4)                                  | 70B (Claude 2)                                | 540B (PaLM 2 Bison)                          | 70B (LLaMA 2)                               | 7B                                            |
| **Strengths**                | - General-purpose.<br>- Multi-tasking.<br>- Long context handling (8k-32k tokens).<br>- Advanced reasoning. | - Emphasizes safety & interpretability.<br>- Strong at language, ethical decisions.<br>- User alignment. | - Good at reasoning.<br>- Stronger at code generation & math.<br>- Handles longer contexts. | - Fine-tuned for specific tasks.<br>- High efficiency for its size.<br>- Open-source for developers. | - High efficiency for smaller deployments.<br>- Optimized for fast inference and performance. |
| **Weaknesses**               | - Expensive to run at scale.<br>- Requires fine-tuning for domain-specific tasks. | - Slightly slower at general knowledge tasks.<br>- Not as widely adopted as GPT-4. | - Limited availability compared to other models.<br>- Can be slower for some tasks. | - Performance not as strong as GPT-4 in open-ended tasks.<br>- Needs custom fine-tuning for specialized tasks. | - Smaller size limits it in comparison to larger models.<br>- Good for simpler tasks, but less knowledge depth. |
| **Special Features**         | - Few-shot learning.<br>- Chain-of-thought prompting.<br>- Extensive API support. | - Ethical reasoning & alignment.<br>- Safety improvements.<br>- Handles nuanced decisions. | - Enhanced capabilities for math and coding.<br>- Stronger multilingual support. | - Open-source availability.<br>- Modifiable by developers.<br>- Efficient use of resources for training. | - Extremely efficient for a smaller model.<br>- Low-cost deployment.<br>- Competitive in performance despite smaller size. |
| **Primary Use Cases**        | - Chatbots.<br>- Complex reasoning tasks.<br>- Coding assistants.<br>- Creative content generation. | - Ethical decision-making.<br>- Customer support.<br>- Legal & compliance use cases. | - Code generation.<br>- Conversational agents.<br>- Research & writing. | - Open-source experiments.<br>- Chatbots for specific domains.<br>- Educational purposes. | - Lightweight chat applications.<br>- Efficient inference tasks.<br>- General knowledge Q&A. |
| **Performance**              | Excellent at multi-tasking, reasoning, and code completion. | Great alignment with user safety, and good for ethical and conversational reasoning. | Excels in code generation, multi-lingual tasks, and research. | Strong at domain-specific tasks with low latency. | Quick inference with solid results despite smaller parameter size. |
| **Cost**                     | High cost, especially for longer contexts (costs based on input/output tokens). | Slightly lower cost than GPT-4, but still significant for long contexts. | Comparable to GPT-4, but typically depends on cloud infrastructure costs. | Lower cost for open-source use cases, no commercial restrictions. | Lower cost due to smaller size and lighter resource requirements. |
| **Context Window Size**      | 8K to 32K tokens, depending on version.        | 75K tokens, optimized for long conversations. | Up to 32K tokens, good for extended chats.   | 4K to 32K tokens, depending on configuration. | ~4K tokens, efficient for shorter conversations. |
| **Availability**             | OpenAI API, used in ChatGPT and enterprise products. | Anthropic API, available for enterprise and research. | Google Cloud AI (Vertex AI), API for developers. | Open-source, available on platforms like Hugging Face. | Available on Hugging Face and open-source libraries. |
| **Best For**                 | - General-purpose chat.<br>- Code assistance.<br>- Complex queries. | - User alignment & ethical AI.<br>- Customer support. | - Code generation.<br>- Research and educational tools. | - Developers who want customizable, open-source models. | - Lightweight tasks with fast, cost-efficient inference. |



## Key Aspects of Open-Weight Models:

| **Aspect**               | **Description**                                                                                                  |
|--------------------------|------------------------------------------------------------------------------------------------------------------|
| **Access to Weights**     | The trained parameters (weights) of the model are made available for download. This allows users to inspect, modify, or fine-tune the model. |
| **Customization**         | Users can modify and adapt the model to their specific use cases, including retraining the model on their own datasets. |
| **Transparency**          | Open-weight models promote transparency, allowing researchers and developers to understand how the model works internally. |
| **Community Contributions** | Open models encourage community contributions, where users can share improvements, benchmarks, or optimizations for the models. |
| **Cost-Efficiency**       | Since users can download and run the models locally, they avoid ongoing API costs that might come with closed-weight models. |
| **Deployment Flexibility** | Users can deploy these models on their infrastructure, ensuring data privacy and reducing reliance on external cloud platforms. |
| **Comparability**         | Open-weight models allow for direct comparisons and benchmarking, enabling the research community to advance AI by sharing insights. |


### Examples of Open-Weight Models:

- **Mistral 7B**: A high-performance, open-weight language model with 7 billion parameters designed for efficient performance in a variety of tasks.
- **Meta's LLaMA 2**: A suite of open-weight models, which Meta (formerly Facebook) released to support academic research and business applications.
- **Bloom**: A large open-weight model developed by BigScience, intended to be a multilingual, open-access alternative to proprietary models.
  
### Use Cases of Open-Weight Models:

- **Custom AI Applications**: Businesses can customize open-weight models for internal chatbots, customer service automation, and data analysis tools.
- **Fine-Tuning for Specialized Tasks**: Researchers and developers can fine-tune the models on industry-specific datasets for more accurate results.
- **Academic Research**: Universities and independent researchers can use open-weight models for developing new AI techniques, evaluating existing methods, and training students.

In contrast, **closed-weight models** like GPT-4 or Google's Bard only offer API access and do not share their internal weights, limiting user flexibility and requiring dependence on third-party platforms for usage.


### **"Download the Model Weights"** and **"Download the Actual Model"** are often used interchangeably but represent slightly different aspects of a machine learning model. Letâ€™s clarify the distinction between the two:

| **Aspect**                   | **Download Model Weights**                                        | **Download Actual Model**                                          |
|------------------------------|------------------------------------------------------------------|--------------------------------------------------------------------|
| **Content**                   | Only the trained parameters (weights)                            | Both the architecture and the weights                              |
| **Dependency**                | Requires pre-defined model architecture to load weights          | Self-contained: includes both architecture and weights             |
| **Usage**                     | Used when you already have the architecture                      | Used when you need everything (architecture + weights)             |
| **File Size**                 | Often smaller because only the weights are downloaded            | Typically larger because it includes architecture and weights      |
| **Example**                   | Downloading weights for LLaMA 2                                  | Downloading the full model from Hugging Face                       |
| **Flexibility**               | More flexible for using different architectures or frameworks    | Easier for direct usage, but less flexible for custom architectures|


## **ideal business use cases** for **Prompt Engineering**, **Retrieval-Augmented Generation (RAG)**, and **LLM Fine-Tuning**:

| **Category**              | **Ideal Business Use Cases**                                                                                   | **Key Benefits**                                                                                            | **When to Use**                                                                                                                                                 |
|---------------------------|----------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Prompt Engineering**     | 1. **Customer Support Automation**: Create a chatbot to answer FAQs using GPT-based models.                    <br> 2. **Content Creation**: Generate blog posts, social media captions, product descriptions.       <br> 3. **Code Generation**: Automatically generate code snippets or SQL queries.                                  <br> 4. **Data Summarization**: Summarize large documents or research papers.                             | - Low cost and fast deployment.                                                                                            <br> - No need for fine-tuning, just task-specific prompting.                                                   | - Use when you need quick, domain-agnostic outputs based on dynamic prompts.                                        <br> - When customization is not deeply required.                                                                     |
| **RAG (Retrieval-Augmented Generation)** | 1. **Knowledge Management Systems**: Create AI to retrieve information from large knowledge bases.  <br> 2. **Legal Document Search**: Generate legal documents by retrieving information from databases.   <br> 3. **Customer Feedback Analysis**: Retrieve insights from user-generated content or reviews. <br> 4. **Internal Support Systems**: Retrieve specific documents like HR or policy-related guidelines. | - Can deal with a larger context of real-time or domain-specific information.<br> - Better for knowledge-driven answers and precise retrieval. | - When your model needs access to up-to-date, company-specific, or large datasets (beyond model's pre-training).<br> - Use when you need precise, fact-based outputs. |
| **LLM Fine-Tuning**        | 1. **Domain-Specific Assistants**: Fine-tune models to give responses specific to your business or industry.<br> 2. **Specialized Medical Assistant**: Fine-tune on medical journals or diagnostic datasets.<br> 3. **Legal AI Assistant**: Train on legal documents to improve specific legal decision-making.<br> 4. **Custom Translation Models**: Train models on specific jargon, tone, or language preferences for businesses.              | - Customizable, with more control over output quality.<br> - Enhanced model performance for specialized domains.<br> - Retains long-term knowledge after fine-tuning. | - When domain-specific customization is required.<br> - If you have sufficient labeled data and computational resources.<br> - When high accuracy is critical. |


# comparison of different LLM (Large Language Model) performance metrics with brief explanations for each:

| **Metric**           | **Explanation**                                                                                           | **Importance**                                                                                                                      | **How Measured**                                                                                                                                                           |
|----------------------|-----------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Model Accuracy**    | Measures how often the LLM's predictions are correct. Accuracy is averaged across all examples in a dataset.| Ensures that the model outputs the right or expected results, crucial in task-specific applications like classification or QA tasks. | Calculated as the ratio of correct predictions to total predictions (e.g., in binary classification or multiple-choice tasks).                                               |
| **Model Coherence**   | Assesses the flow and naturalness of the model's generated text, checking if it reads like human language. | Coherence is essential for natural language generation (NLG), such as storytelling, summarization, and dialogue systems.             | Evaluated through human judgment or automatic scoring based on linguistic structure and logical flow of text over a given passage or paragraph.                             |
| **Model Groundedness**| Determines how well the model can produce factually accurate or logically sound content based on known facts.| Critical for applications where factual correctness is essential, such as news summarization, scientific writing, or knowledge bases.| Groundedness can be evaluated by comparing generated text against trusted reference sources or databases.                                                                   |
| **Model Fluency**     | Measures the language proficiency in the generated response (grammar, vocabulary, syntax).                 | Essential for applications where output quality and clarity are critical, such as customer support and dialogue agents.              | Fluency is usually evaluated through human annotations or automated grammatical checks, ensuring proper sentence structure, punctuation, and word choice.                    |
| **Model Relevance**   | Evaluates how relevant the output is to the input prompt or query.                                         | Ensures the generated content is meaningful and contextually aligned with the user's query or task.                                  | Relevance is assessed by comparing how closely the generated response relates to the context of the input, often through human evaluation or automated scoring algorithms.   |
| **Model Similarity**  | Measures how closely the generated response matches the reference data (ground truth) in meaning and content.| Important in tasks like machine translation, summarization, and paraphrasing, where high similarity to ground truth is desired.      | Calculated using metrics like cosine similarity, BLEU score, or ROUGE to assess the similarity between the generated output and the reference text.                         |



# Reference
- https://github.com/vivek-bombatkar/generative-ai-for-beginners?tab=readme-ov-file




Resources:
- [microsoft / AutoGen / development of LLM applications using multiple agents  ](https://microsoft.github.io/autogen/docs/Getting-Started)
