# Generative AI & Deep Learning Notes
### My Learning Notes from [NUS generative-ai-fundamentals-to-advanced-techniques-programme](https://nus.comp.emeritus.org/generative-ai-fundamentals-to-advanced-techniques-programme)

## Table of Contents
- [Learning Paradigms in Machine Learning](#learning-paradigms-in-machine-learning)
- [From Brains to Artificial Neural Networks](#from-brains-to-artificial-neural-networks)
- [Convolutional Neural Networks](#convolutional-neural-networks)
- [Transformers and Attention Mechanisms](#transformers-and-attention-mechanisms)
- [Transformer Model Families](#transformer-model-families)
- [Alignment, Reliability, and Knowledge Grounding](#alignment-reliability-and-knowledge-grounding)
- [Multimodal and Generalist Models](#multimodal-and-generalist-models)
- [Useful Links](#useful-links)



## Learning Paradigms in Machine Learning

### Reinforcement Learning vs Supervised Learning

- **Supervised Learning:** A learning paradigm where a model is trained on labelled input-output pairs and receives direct feedback on errors. îˆ€citeîˆ‚turn20search0îˆ  
- **Reinforcement Learning (RL):** A learning paradigm where an agent learns by interacting with an environment using rewards and penalties. îˆ€citeîˆ‚turn5search0îˆ  

#### Supervised Learning
1. Collect labelled data. îˆ€citeîˆ‚turn20search0îˆ  
2. Split into training and validation sets. îˆ€citeîˆ‚turn20search0îˆ  
3. Train model to minimize loss. îˆ€citeîˆ‚turn20search0îˆ  
4. Evaluate performance. îˆ€citeîˆ‚turn20search0îˆ  

#### Reinforcement Learning
1. Initialize agent and environment. îˆ€citeîˆ‚turn5search0îˆ  
2. Perform actions based on current policy. îˆ€citeîˆ‚turn5search0îˆ  
3. Receive reward feedback. îˆ€citeîˆ‚turn5search0îˆ  
4. Update policy using reward signal. îˆ€citeîˆ‚turn5search0îˆ  
5. Repeat until convergence. îˆ€citeîˆ‚turn5search0îˆ  

#### Examples
- **Supervised Learning:** Image classification, spam detection, price prediction. îˆ€citeîˆ‚turn20search0îˆ  
- **Reinforcement Learning:** Game playing, robotics control, recommendation strategies with delayed rewards. îˆ€citeîˆ‚turn5search0îˆ‚turn2search3îˆ  

### Unsupervised Learning

- **Unsupervised Learning:** A branch of machine learning where models learn patterns and relationships from unlabelled data without predefined outputs. îˆ€citeîˆ‚turn20search1îˆ  

#### Examples

##### Clustering Problems
- Customer segmentation îˆ€citeîˆ‚turn20search1îˆ  
- Image segmentation îˆ€citeîˆ‚turn20search1îˆ  

##### Dimensionality Reduction Problems
- Principal Component Analysis (PCA) îˆ€citeîˆ‚turn4search12îˆ  
- t-SNE îˆ€citeîˆ‚turn5search1îˆ  

##### Anomaly Detection
- Fraud detection îˆ€citeîˆ‚turn20search7îˆ  

##### Generative Models (partly unsupervised)
- Autoencoders îˆ€citeîˆ‚turn5search3îˆ  
- GANs (Generative Adversarial Networks) îˆ€citeîˆ‚turn5search2îˆ  

##### Market Basket Analysis
- Identifying frequently co-occurring items îˆ€citeîˆ‚turn20search6îˆ‚turn20search22îˆ  

## From Brains to Artificial Neural Networks

### The Human Brain and Neural Complexity

#### Overview
The human brain is an incredibly complex organ and one of natureâ€™s greatest engineering marvels.  
The **neocortex** plays a key role in higher cognitive functions such as:
- ğŸ§  Reasoning  
- ğŸ‘ï¸ Perception  
- ğŸ¤” Decision-making  
- ğŸ—£ï¸ Language  
Its massive connectivity gives the brain extraordinary computational power. îˆ€citeîˆ‚turn4search5îˆ‚turn7search6îˆ  

#### Neurons and Synapses
- The brain contains around **100 billion neurons** ğŸ§© îˆ€citeîˆ‚turn4search5îˆ  
- Each neuron connects to **1,000â€“10,000** other neurons through **synapses** îˆ€citeîˆ‚turn7search6îˆ‚turn7search2îˆ  
- Synapses transmit information via âš¡ electrical and ğŸ§ª chemical signals îˆ€citeîˆ‚turn7search2îˆ  
- The neocortex alone is estimated to have about **500 trillion synapses**, forming a massive biological network capable of learning and thought îˆ€citeîˆ‚turn7search2îˆ‚turn8search0îˆ  

> Note (clarification added): Published estimates vary by method and definition; widely cited totals include ~86 billion neurons for the whole brain and on the order of 10^14 synapses overall, with neocortex synapse counts reported in the ~10^14 range as well. îˆ€citeîˆ‚turn4search5îˆ‚turn7search2îˆ  

#### Neural Wiring and Efficiency
- The neocortex contains roughly **300 million feet (â‰ˆ91,440 km)** of neural wiring îˆ€citeîˆ‚turn8search0îˆ  
- This wiring is compacted into a volume of about **1.5 quarts (â‰ˆ1.4 liters)** ğŸ¤¯ îˆ€citeîˆ‚turn8search0îˆ  
- Such efficiency is achieved through several biological optimizations: îˆ€citeîˆ‚turn8search0îˆ‚turn4search5îˆ  

##### Folding of the Cortex
- The brain surface is folded into **gyri** (ridges) and **sulci** (grooves)  
- Folding increases surface area without increasing overall volume îˆ€citeîˆ‚turn8search0îˆ  

##### Myelination
- Axons are coated with **myelin**, a fatty insulating layer  
- Myelin speeds up signal transmission and reduces energy usage îˆ€citeîˆ‚turn4search15îˆ  

##### Specialized Networks
- The brain is organized into specialized functional areas, such as:
  - ğŸ‘€ Visual cortex
  - âœ‹ Motor cortex
  - ğŸ§  Prefrontal cortex
- Specialization minimizes unnecessary wiring and improves processing speed îˆ€citeîˆ‚turn8search0îˆ‚turn4search5îˆ  

### Neural Networks and Artificial Neural Networks

- Neural networks are inspired by biological brains. îˆ€citeîˆ‚turn5search0îˆ  
- Artificial neurons approximate real neurons. îˆ€citeîˆ‚turn5search0îˆ  
- ANNs are networks of artificial neurons. îˆ€citeîˆ‚turn5search0îˆ  
- ANNs are simplified models of brain functionality. îˆ€citeîˆ‚turn5search0îˆ  
- Practically, ANNs are parallel computational systems. îˆ€citeîˆ‚turn5search0îˆ  

#### Definitions
- **Neural Networks (NNs):** Networks of neurons similar to those found in biological brains. îˆ€citeîˆ‚turn5search0îˆ  
- **Artificial Neurons:** Crude approximations of biological neurons, implemented as mathematical or software constructs. îˆ€citeîˆ‚turn5search0îˆ  
- **Artificial Neural Networks (ANNs):** Networks of artificial neurons that approximate certain functions of real brains. îˆ€citeîˆ‚turn5search0îˆ  

### Biological vs Artificial Neurons

#### Biological Neurons
- Biological neurons have **synaptic gaps** of varying strengths ğŸ”— îˆ€citeîˆ‚turn7search2îˆ  
- These synapses connect to the **soma (cell body)** ğŸ§  îˆ€citeîˆ‚turn7search2îˆ  
- Signal strength depends on synaptic weight and connectivity îˆ€citeîˆ‚turn7search2îˆ  
- Information flows via:
  - ğŸŒ¿ Dendrites (input)
  - ğŸ§  Cell body (integration)
  - âš¡ Axon (signal transmission)
  - ğŸ”š Axon terminals (output) îˆ€citeîˆ‚turn7search2îˆ  

#### Artificial Neurons
- Artificial neurons replace synapses with **numerical inputs** îˆ€citeîˆ‚turn5search0îˆ  
- Inputs can come from:
  - Other neurons
  - Sensors
  - Data features
  - Variables îˆ€citeîˆ‚turn5search0îˆ  
- Core operations include:
  - â• Weighted sum (Î£)
  - ğŸ“‰ Activation / threshold function îˆ€citeîˆ‚turn5search0îˆ  

#### Computational Power
- The neocortex contains about **500 trillion synapses** operating **in parallel** îˆ€citeîˆ‚turn8search0îˆ‚turn7search2îˆ  
- Enables massive information processing and storage simultaneously îˆ€citeîˆ‚turn7search2îˆ  
- The human brain operates on roughly **20 watts of power** ğŸ’¡ îˆ€citeîˆ‚turn7search1îˆ‚turn4search8îˆ  
- This is far more **energy-efficient** than modern supercomputers îˆ€citeîˆ‚turn7search1îˆ  

#### Implications for Intelligence and Learning
- High neuron density and interconnectivity enable human intelligence îˆ€citeîˆ‚turn4search5îˆ  
- **Plasticity** allows neural connections to reorganize with:
  - ğŸ“š Learning
  - ğŸ§  Experience
  - ğŸ©¹ Recovery from injury îˆ€citeîˆ‚turn4search5îˆ  
- This adaptability is central to skill acquisition and cognition îˆ€citeîˆ‚turn4search5îˆ  

#### Artificial Neural Networks (ANNs)
- ANNs are inspired by biological neural systems îˆ€citeîˆ‚turn5search0îˆ  
- They aim to approximate learning and decision-making îˆ€citeîˆ‚turn5search0îˆ  
- While powerful, they are far less energy-efficient than the human brain îˆ€citeîˆ‚turn7search1îˆ  
- The neocortex remains a benchmark for efficient computation and learning îˆ€citeîˆ‚turn7search1îˆ‚turn4search5îˆ  

### Artificial Neural Networks. Why?

- ğŸ§® **Extremely powerful computational devices**  
  - Turing-equivalent universal computers îˆ€citeîˆ‚turn5search0îˆ  
- âš¡ **Massive parallelism**  
  - Many simple units operate simultaneously, making computation efficient îˆ€citeîˆ‚turn5search0îˆ  
- ğŸ“š **Learning and generalization**  
  - Learn directly from training data  
  - No need for carefully handcrafted rules or designs îˆ€citeîˆ‚turn20search0îˆ  
- ğŸ›¡ï¸ **Fault-tolerant and noise-tolerant**  
  - Performance degrades gracefully even with imperfect data or failures îˆ€citeîˆ‚turn5search0îˆ  
- ğŸ§  **Beyond symbolic systems**  
  - Can do everything a symbolic or logic-based system can, and more îˆ€citeîˆ‚turn5search0îˆ  
- ğŸ“Š **Excellent with unstructured data**  
  - Particularly strong with:
    - ğŸ“ Text  
    - ğŸ–¼ï¸ Images  
    - ğŸ”Š Audio  
    - Other semi-structured data îˆ€citeîˆ‚turn5search0îˆ  

## Convolutional Neural Networks

### Deep Convolutional Neural Networks (CNNs)

#### Definition
**Deep Convolutional Neural Networks (CNNs)** are a specialized type of neural network designed to process **structured, grid-like data**, especially **images** ğŸ–¼ï¸. îˆ€citeîˆ‚turn4search1îˆ  

#### Key characteristics
- Designed for **spatial data** arranged in grids îˆ€citeîˆ‚turn4search1îˆ  
- Combine:
  - ğŸ§  **Convolutional layers** for feature extraction
  - ğŸ”— **Fully connected layers** for decision-making îˆ€citeîˆ‚turn4search1îˆ  
- Inspired by the **human visual system** îˆ€citeîˆ‚turn4search1îˆ  
- Highly effective when **spatial hierarchies** matter îˆ€citeîˆ‚turn4search1îˆ  

#### Core building blocks
- ğŸŸ¨ **Input**  
- ğŸŸ© **Convolutional Layer**. Extracts local features (edges, textures)
- ğŸŸ¦ **Pooling Layer**. Reduces spatial size and computation
- ğŸ§ª **Activation Function**. Adds non-linearity
- ğŸ”µ **Fully Connected Layer**. Performs classification or prediction
- ğŸ¯ **Output** îˆ€citeîˆ‚turn4search1îˆ  

#### Why CNNs work well
- Learn **hierarchical features**. from simple edges to complex shapes îˆ€citeîˆ‚turn4search1îˆ  
- Preserve **spatial relationships** in data îˆ€citeîˆ‚turn4search1îˆ  
- Particularly strong for **vision-based tasks** îˆ€citeîˆ‚turn4search1îˆ  

#### Typical use cases
- Image classification
- Object detection
- Image segmentation
- Visual pattern recognition îˆ€citeîˆ‚turn4search1îˆ  

#### Advantages of CNNs
- âš™ï¸ **Automatic Feature Extraction**  
  - CNNs learn features directly from raw data  
  - No manual feature engineering required îˆ€citeîˆ‚turn4search1îˆ  
- ğŸ” **Parameter Sharing**  
  - Same filters are reused across the image  
  - Fewer parameters than fully connected networks îˆ€citeîˆ‚turn4search1îˆ  
- ğŸ“ **Translation Invariance**  
  - Recognize patterns (edges, shapes, objects) regardless of position in the image îˆ€citeîˆ‚turn4search1îˆ  
- ğŸ“ **Efficient for High-Dimensional Data**  
  - Scales well to large images and datasets îˆ€citeîˆ‚turn4search1îˆ  
- ğŸ† **State-of-the-Art Performance**  
  - Top results in:
    - Image classification
    - Object detection
    - Image segmentation îˆ€citeîˆ‚turn4search1îˆ  
- ğŸŒ **Adaptability to Diverse Domains**  
  - Can be applied to:
    - ğŸ–¼ï¸ Images
    - ğŸ”Š Audio spectrograms
    - â±ï¸ Time-series data  
  - Requires minimal architectural changes îˆ€citeîˆ‚turn4search1îˆ  

#### Limitations of CNNs
- ğŸ’» **Computationally Intensive**  
  - Training requires powerful hardware (GPUs, TPUs) îˆ€citeîˆ‚turn4search1îˆ  
- ğŸ“Š **Data Hungry**  
  - Needs large labelled datasets for good performance  
  - Data collection and annotation can be expensive îˆ€citeîˆ‚turn4search1îˆ  
- ğŸ” **Lack of Interpretability**  
  - Acts as a black-box model  
  - Difficult to understand or debug decisions îˆ€citeîˆ‚turn4search1îˆ  
- ğŸ“‰ **Overfitting Risk**  
  - Without proper regularisation, models may memorise training data îˆ€citeîˆ‚turn4search1îˆ  
- ğŸ›ï¸ **Sensitivity to Hyperparameters**  
  - Performance depends heavily on:
    - Architecture
    - Learning rate
    - Other tuning choices îˆ€citeîˆ‚turn4search1îˆ  

## Transformers and Attention Mechanisms

### Attention Mechanism in Transformers

#### Attention
At the core of modern NLP lies **attention**.  
It allows models to **focus**, not memorise.  
Instead of treating all words equally, attention helps the model decide **what matters most** in context. îˆ€citeîˆ‚turn15view0îˆ  

A commonly used formulation is scaled dot-product attention:  
**Attention(Q, K, V) = softmax(QKáµ€ / âˆšdâ‚–) Â· V**. îˆ€citeîˆ‚turn15view0îˆ  

#### Query, Key, and Value (QKV)

To understand attention, everything revolves around three components:

- ğŸ” **Query (Q)**  
  - Represents the **current word or token**
  - Think of it as a word *asking* for relevant context îˆ€citeîˆ‚turn15view0îˆ  

- ğŸ—ï¸ **Key (K)**  
  - Represents **all words in the input sequence**
  - Each word has a key used to measure relevance to the query îˆ€citeîˆ‚turn15view0îˆ  

- ğŸ“¦ **Value (V)**  
  - Contains the **actual information** (embeddings)
  - This is the content passed forward once relevance is determined îˆ€citeîˆ‚turn15view0îˆ  

#### How Attention Works (Step-by-Step)

- Compute **attention scores** using dot products between Query and Keys. îˆ€citeîˆ‚turn15view0îˆ  
- Scale scores by **âˆšdâ‚–** for stability. îˆ€citeîˆ‚turn15view0îˆ  
- Apply **softmax** to get a probability distribution over tokens. îˆ€citeîˆ‚turn15view0îˆ  
- Take the **weighted sum of Values** to produce a context-aware representation. îˆ€citeîˆ‚turn15view0îˆ  

#### Why Attention Matters
- Focuses on **relevant words**, regardless of position îˆ€citeîˆ‚turn15view0îˆ  
- Handles **long-range dependencies** îˆ€citeîˆ‚turn15view0îˆ  
- Essential for understanding meaning in complex sentences îˆ€citeîˆ‚turn15view0îˆ  

### Multi-Head Attention

#### What is Multi-Head Attention?
- Instead of one attention mechanism, the model uses **multiple heads**
- Each head attends to **different aspects** of the sequence îˆ€citeîˆ‚turn15view0îˆ  

#### How It Works
- Input is split into multiple Q, K, V sets  
- Each head computes attention **independently**  
- Outputs are:
  - Concatenated
  - Passed through a linear layer  
â¡ï¸ Result. richer and more expressive representations îˆ€citeîˆ‚turn15view0îˆ  

### Feedforward Networks (FFN)

#### Role of Feedforward Networks
- Applied **after multi-head attention**
- Operates **independently on each position** îˆ€citeîˆ‚turn15view0îˆ  

#### Structure
- Linear layer
- ReLU activation
- Linear layer îˆ€citeîˆ‚turn15view0îˆ  

#### Purpose
- Captures **more abstract patterns**
- Refines representations beyond word-to-word relationships
- Output is passed to the next transformer layer îˆ€citeîˆ‚turn15view0îˆ  

### Transformer Architecture. Big Picture

- ğŸ”¡ Embeddings îˆ€citeîˆ‚turn15view0îˆ  
- ğŸ” Self-attention (QKV) îˆ€citeîˆ‚turn15view0îˆ  
- ğŸ”€ Multi-head attention îˆ€citeîˆ‚turn15view0îˆ  
- ğŸ§ª Feedforward networks îˆ€citeîˆ‚turn15view0îˆ  
- ğŸ”„ Stacked layers for depth îˆ€citeîˆ‚turn15view0îˆ  

Together, these components allow transformers to:
- Understand context deeply
- Scale efficiently
- Generate high-quality, human-like text îˆ€citeîˆ‚turn15view0îˆ  

### Training a Transformer
- ğŸ”¡ Data preprocessing. tokenisation and embeddings îˆ€citeîˆ‚turn15view0îˆ‚turn2search4îˆ  
- ğŸ“ Positional encodings added for sequence order îˆ€citeîˆ‚turn15view0îˆ  
- ğŸ² Random weight initialisation îˆ€citeîˆ‚turn15view0îˆ  
- ğŸ” Training via **backpropagation** îˆ€citeîˆ‚turn15view0îˆ  
- ğŸ›ï¸ Hyperparameters tuned:
  - Learning rate
  - Batch size îˆ€citeîˆ‚turn15view0îˆ‚turn2search4îˆ  
- Goal. minimise loss and optimise performance îˆ€citeîˆ‚turn15view0îˆ  

### Computational Challenges
- Large models are **computationally intensive** îˆ€citeîˆ‚turn15view0îˆ‚turn1search7îˆ  
- Example:
  - GPT-3 has **175 billion parameters** îˆ€citeîˆ‚turn1search7îˆ  
- CPUs are insufficient for training
- Requires:
  - âš¡ GPUs
  - ğŸš€ TPUs îˆ€citeîˆ‚turn1search7îˆ‚turn17view0îˆ  
- Computational needs have grown **exponentially**
- Hardware advances are critical for progress îˆ€citeîˆ‚turn12search7îˆ  

### Transformer Variants
- Transformers excel at sequential data
- Variants optimise performance for specific tasks îˆ€citeîˆ‚turn3search10îˆ‚turn4search0îˆ  

#### Hybrid Architectures

##### CNN + Transformer
- Common in computer vision
- CNN captures **local spatial features**
- Transformer captures **long-range dependencies** îˆ€citeîˆ‚turn4search1îˆ‚turn15view0îˆ  
- Used in:
  - Vision Transformers (ViTs)
  - ResNet hybrids îˆ€citeîˆ‚turn4search1îˆ  

##### RNN + Transformer
- Useful for:
  - ğŸ—£ï¸ Speech recognition
  - â±ï¸ Time-series forecasting îˆ€citeîˆ‚turn15view0îˆ  
- RNN handles local sequences
- Transformer handles global context îˆ€citeîˆ‚turn15view0îˆ  

##### Benefits
- Reduced computational cost
- Better efficiency and scalability
- Strong performance on complex tasks îˆ€citeîˆ‚turn3search10îˆ‚turn4search0îˆ  

#### Efficient Transformers (Long Sequences)
- Standard transformers scale **quadratically** with sequence length due to self-attention îˆ€citeîˆ‚turn3search10îˆ‚turn3search3îˆ‚turn4search0îˆ  
- Problematic for long documents
- Sparse and efficient variants address this îˆ€citeîˆ‚turn3search10îˆ‚turn4search0îˆ  

##### Longformer
- Uses attention patterns combining local + global information
- Scales more linearly for long documents îˆ€citeîˆ‚turn3search10îˆ‚turn3search2îˆ  
- Ideal for:
  - Document-level QA
  - Summarisation îˆ€citeîˆ‚turn3search10îˆ  

##### Linformer
- Approximates attention via low-rank structure
- Reduces memory and inference cost for long sequences îˆ€citeîˆ‚turn3search3îˆ  

##### Reformer
- Optimised for memory efficiency
- Uses locality-sensitive hashing (LSH) attention and reversible layers îˆ€citeîˆ‚turn4search0îˆ  

## Transformer Model Families

### BERT vs GPT vs T5

#### BERT
- **Bidirectional Encoder Representations from Transformers**
- Understands text in **both directions** îˆ€citeîˆ‚turn17view0îˆ  
- Best suited for:
  - ğŸ˜Š Sentiment analysis
  - ğŸ·ï¸ Text classification
  - â“ Question answering îˆ€citeîˆ‚turn17view0îˆ  

#### GPT
- **Generative Pre-trained Transformer**
- **Autoregressive**. predicts the next word in a sequence îˆ€citeîˆ‚turn15view0îˆ‚turn1search7îˆ  
- Ideal for:
  - ğŸ’¬ Conversational AI
  - âœï¸ Content generation
  - ğŸ’» Code generation îˆ€citeîˆ‚turn1search7îˆ  

#### T5
- **Text-to-Text Transfer Transformer**
- Treats **all NLP tasks as text-to-text** îˆ€citeîˆ‚turn2search4îˆ  
- Highly versatile for:
  - ğŸŒ Translation
  - ğŸ§¾ Summarisation
  - â“ Question answering îˆ€citeîˆ‚turn2search4îˆ  

### BERT limitations and common variants

#### Limitations of BERT (encoder-only)
- **Context window limit:** BERT is commonly pre-trained with an input context window up to **512 tokens**. îˆ€citeîˆ‚turn18view0îˆ‚turn0search13îˆ  
- **Dimensionality cost:** common configurations include hidden sizes like **768** (BERT-base), which affects memory and compute. îˆ€citeîˆ‚turn0search13îˆ‚turn19search15îˆ  
- **No true free-form generation:** BERT is trained with a masked prediction objective (and NSP in the original version), so it excels at understanding/representation learning rather than left-to-right generation. îˆ€citeîˆ‚turn17view0îˆ‚turn18view0îˆ  

#### Variants mentioned in the notes and discussion
- **RoBERTa:** improved BERT pretraining by changing key training choices (for example, removing NSP and using dynamic masking and more data/compute). îˆ€citeîˆ‚turn0search6îˆ‚turn0search10îˆ  
- **ELECTRA:** replaces masked-token prediction with **replaced token detection** (discriminator predicts whether a token was replaced by a generator). îˆ€citeîˆ‚turn0search3îˆ‚turn0search7îˆ  
- **DistilBERT:** uses knowledge distillation to create a smaller, faster model while preserving much of BERTâ€™s performance. îˆ€citeîˆ‚turn11search0îˆ  
- **ALBERT:** reduces parameters (e.g., factorized embeddings and parameter sharing) to improve efficiency. îˆ€citeîˆ‚turn11search5îˆ  
- **SpanBERT:** masks contiguous spans and trains objectives tailored to span representations. îˆ€citeîˆ‚turn11search6îˆ  
- **CodeBERT:** pre-trained for natural language + programming language tasks (bimodal NLâ€“PL). îˆ€citeîˆ‚turn11search3îˆ  

### GPT models and how ChatGPT works

#### Model Scale Comparison

| Model   | Number of Parameters |
|--------|----------------------|
| GPT-1  | 117M |
| GPT-2  | 1.5B |
| GPT-3  | 175B |
| GPT-3.5 | GPT-3 + ~6B |
| GPT-4  | ~1.7T |

Notes/clarifications added:
- GPT-2 (1.5B) and GPT-3 (175B) parameter counts are documented in their respective technical reports. îˆ€citeîˆ‚turn1search0îˆ‚turn1search7îˆ  
- Some commonly repeated values for â€œGPT-3.5â€ and â€œGPT-4 parameter countâ€ are **not officially disclosed**. The GPT-4 technical report explicitly states it does not provide details such as model size. îˆ€citeîˆ‚turn14view0îˆ  

#### How ChatGPT Works (End-to-End Flow)

â¡ï¸ **Pre-training**  
- Trained on massive internet text
- Learns to predict the **next token**
- Captures grammar, facts, and language patterns îˆ€citeîˆ‚turn1search7îˆ‚turn13view0îˆ  

â¡ï¸ **Fine-tuning**  
- Uses datasets reviewed by **human trainers**
- Learns to generate safer and more helpful responses
- Generalises from human feedback îˆ€citeîˆ‚turn1search1îˆ‚turn1search5îˆ  

â¡ï¸ **Input Processing**  
- User input is **tokenised** into words or subwords
- Tokens are fed into the transformer model îˆ€citeîˆ‚turn2search4îˆ‚turn13view0îˆ  

â¡ï¸ **Contextual Understanding**  
- Maintains conversation history ğŸ§ 
- Transformer architecture models long-range dependencies
- Enables context-aware responses îˆ€citeîˆ‚turn15view0îˆ‚turn13view0îˆ  

â¡ï¸ **Response Generation**  
- Predicts next tokens based on learned patterns
- Produces coherent, human-like text îˆ€citeîˆ‚turn15view0îˆ‚turn13view0îˆ  

â¡ï¸ **Sampling and Optimisation**  
- Uses probabilistic sampling ğŸ²
- Adds controlled randomness for natural responses
- Safety techniques reduce harmful outputs îˆ€citeîˆ‚turn6search0îˆ‚turn1search1îˆ‚turn13view0îˆ  

â¡ï¸ **Post-processing**  
- Removes special tokens and formatting
- Final response is shown to the user ğŸ’¬ îˆ€citeîˆ‚turn13view0îˆ  

### Encoderâ€“decoder models and key examples

#### Encoder-decoder models
Encoderâ€“decoder models are a fundamental architecture in modern deep learning. These models bring together an encoder and a decoder, enabling efficient processing of input data while generating meaningful output. îˆ€citeîˆ‚turn15view0îˆ‚turn2search4îˆ  

- Start with the encoder, which takes the input and processes it to capture its contextual meaning. It transforms the data into a structured representation that the model can understand. îˆ€citeîˆ‚turn15view0îˆ  
- Then comes the decoder, which uses that structured information to generate text that is natural and coherent. Unlike simple text generation models that produce output sequentially, encoder-decoder models maintain logical consistency by referencing the complete context provided by the encoder. îˆ€citeîˆ‚turn15view0îˆ  

#### Applications of encoderâ€“decoder models
Encoderâ€“decoder models power a variety of real-world applications including:
- Machine translation to convert text from one language to another îˆ€citeîˆ‚turn15view0îˆ‚turn2search4îˆ  
- Text summarisation, extracting the key points while preserving the meaning îˆ€citeîˆ‚turn2search4îˆ‚turn2search2îˆ  
- Caption generation, generating textual descriptions for images or videos îˆ€citeîˆ‚turn3search1îˆ  

#### BART
BART (Bidirectional and Auto-Regressive Transformer) is a hybrid model that integrates the strengths of both BERT and GPT, making it highly effective for tasks requiring text reconstruction and controlled generation. îˆ€citeîˆ‚turn2search1îˆ  

- It employs a bidirectional encoding process (similar to BERT) for comprehensive contextual understanding. îˆ€citeîˆ‚turn2search1îˆ  
- It uses denoising objectives such as span corruption / text infilling (replacing spans with a single mask token), forcing reconstruction of phrases while maintaining coherence. îˆ€citeîˆ‚turn2search1îˆ  
- On the decoding side, BART adopts autoregressive generation (similar to GPT), where tokens are generated one at a time while conditioning on the encoded input. îˆ€citeîˆ‚turn2search1îˆ‚turn15view0îˆ  

##### BARTâ€™s Denoising Process
BART employs a denoising autoencoder approach, where input data is deliberately corrupted before being passed through the model. îˆ€citeîˆ‚turn2search1îˆ  

Noise-insertion techniques described in the notes:
- Token masking replaces random words with a special [MASK] token. îˆ€citeîˆ‚turn2search1îˆ  
- Token deletion removes entire words from the sequence. îˆ€citeîˆ‚turn2search1îˆ  
- Text infilling (span corruption) replaces entire spans of text with a single [MASK] token. îˆ€citeîˆ‚turn2search1îˆ  

This training strategy makes BART robust to noisy or incomplete inputs (e.g., imperfect formatting or missing spans) and supports tasks like summarisation, paraphrasing, translation, and text completion. îˆ€citeîˆ‚turn2search1îˆ  

#### PEGASUS
PEGASUS (Pre-training with Extracted Gap-sentences for Abstractive Summarisation) is specifically designed to enhance text summarisation by masking entire sentences rather than individual tokens during pretraining. îˆ€citeîˆ‚turn2search2îˆ‚turn2search10îˆ  

Key points from the notes:
- Entire sentences (instead of tokens) are masked during pretraining. îˆ€citeîˆ‚turn2search2îˆ  
- Sentences may be randomly identified, though the method focuses on removing â€œimportantâ€ sentences (gap sentence generation). îˆ€citeîˆ‚turn2search2îˆ  
- These are identified to be ones with high similarity to the rest of the document (intuitively encouraging summary-like targets). îˆ€citeîˆ‚turn2search2îˆ  

PEGASUS achieves strong summarisation performance and can require minimal fine-tuning for high-quality abstractive summaries. îˆ€citeîˆ‚turn2search2îˆ‚turn2search10îˆ  

#### T5
T5 (Text-to-Text Transfer Transformer) reformulates all NLP tasks into a text-to-text format, using an encoder-decoder architecture for tasks like translation, summarisation, and Q&A. îˆ€citeîˆ‚turn2search4îˆ  

Text to Text
- Indicative of the types of input and output to be expected îˆ€citeîˆ‚turn2search4îˆ  
- Encoder model used to get information for input text îˆ€citeîˆ‚turn2search4îˆ  
- Decoder model used to generate output text îˆ€citeîˆ‚turn2search4îˆ  

Transfer Transformer
- Transformer capable of employing transfer learning îˆ€citeîˆ‚turn2search4îˆ  
- Allows for multiple NLP tasks to be accomplished by the model:
  - Translation
  - Summarisation
  - Q&A îˆ€citeîˆ‚turn2search4îˆ  

T5 uses task prefixes (instructions) to unify workflows across tasks (e.g., â€œtranslate â€¦â€, â€œsummarize â€¦â€). îˆ€citeîˆ‚turn2search4îˆ  

## Alignment, Reliability, and Knowledge Grounding

### GPT and Reinforcement Learning

GPT models, built on deep learning, have revolutionised language understanding and generation by predicting text patterns with remarkable fluency. Reinforcement Learning (RL), on the other hand, empowers systems to learn through trial and error, optimising actions for long-term rewards. Together, they unlock new frontiers in adaptive, intelligent decision-making and human-like interactions. îˆ€citeîˆ‚turn5search0îˆ‚turn1search7îˆ  

### Reinforcement Learning from Human Feedback (RLHF)

Reinforcement Learning from Human Feedback (RLHF) enhances GPTâ€™s ability to generate not just human-like text but also reliable and contextually appropriate responses. îˆ€citeîˆ‚turn1search1îˆ‚turn13view0îˆ  

Why RLHF is used (as captured in the notes and discussion):
- GPT can produce fluent language, but a pure Transformer architecture does not *inherently* verify factual accuracy or suitability of outputs. îˆ€citeîˆ‚turn13view0îˆ‚turn1search1îˆ  
- Example (preserved): â€œUnder Augustus, the Roman Empire came to [MASK]â€ â€” GPT alone may not â€œknowâ€ which completion is historically correct without grounding or reliable internal knowledge. îˆ€citeîˆ‚turn13view0îˆ‚turn1search1îˆ  

Typical RLHF-style pipeline:
- A model is fine-tuned using a **reward model** (often a transformer trained on human preference rankings) that prioritizes more useful outputs. îˆ€citeîˆ‚turn1search1îˆ‚turn1search5îˆ  
- Human reviewers evaluate and correct a subset of responses, reinforcing high-quality and informative text generation. îˆ€citeîˆ‚turn1search1îˆ‚turn1search5îˆ  

### Retrieval-Augmented Generation (RAG)

Retrieval-Augmented Generation (RAG) enhances GPT by integrating external knowledge sources, addressing its limitations with fixed training data. îˆ€citeîˆ‚turn1search2îˆ‚turn1search6îˆ  

Key points (preserved and clarified):
- ChatGPT generates responses off fixed training data. îˆ€citeîˆ‚turn13view0îˆ  
- Without external sources, it struggles with real-time updates, niche topics, or retrieving specific factual details. îˆ€citeîˆ‚turn1search2îˆ‚turn13view0îˆ  
- RAG retrieves relevant documents or data (e.g., knowledge bases, web snapshots, or indexed corpora), feeding them into the model as additional context for response generation. îˆ€citeîˆ‚turn1search2îˆ‚turn1search6îˆ  
- This approach improves accuracy, reduces hallucinations, and enables up-to-date, domain-specific answers. îˆ€citeîˆ‚turn1search2îˆ‚turn13view0îˆ  
- Combining GPT-style models with retrieval creates more reliable, informed, and adaptable AI systems for business and research applications. îˆ€citeîˆ‚turn1search2îˆ‚turn1search6îˆ  

### Zero-Shot Learning (ZSL)

Zero-Shot Learning (ZSL) enables GPT to perform tasks without additional training, relying solely on its extensive pretraining. Instead of requiring labeled examples or fine-tuning: îˆ€citeîˆ‚turn1search7îˆ  

- GPT leverages its extensive pre-training to perform tasks without additional training, generating relevant outputs directly from prompt instruction. îˆ€citeîˆ‚turn1search7îˆ  
- Enhanced flexibility: allows adaptation to new tasks without extra fine-tuning. îˆ€citeîˆ‚turn1search7îˆ  
- Streamlined workflow: reduces the need for task-specific fine-tuning. îˆ€citeîˆ‚turn1search7îˆ  
- Can be more efficient for real-time applicationsâ€”enhancing overall productivity. îˆ€citeîˆ‚turn1search7îˆ  

### Model Temperature

Temperature is a parameter that controls the randomness of token selection during inference (commonly described as scaling logits before softmax). îˆ€citeîˆ‚turn6search0îˆ  

Notes (preserved):
- Future tokens picked via probability distribution îˆ€citeîˆ‚turn6search0îˆ  
- Higher probability = higher chance of selection îˆ€citeîˆ‚turn6search0îˆ  
- Temperature scaling adjusts randomness in word selection. îˆ€citeîˆ‚turn6search0îˆ  
- Tuning temperature balances precision and creativity in generated text. îˆ€citeîˆ‚turn6search0îˆ  

### Challenges in Text Generation

#### Hallucinations
- Generates plausible but incorrect information
- Caused by missing or weak training signals îˆ€citeîˆ‚turn13view0îˆ  

#### Bias
- Models inherit biases from training data
- Can reflect:
  - Gender bias
  - Cultural bias
  - Racial bias îˆ€citeîˆ‚turn13view0îˆ  

#### Ethics
- Risks include:
  - Misinformation
  - Plagiarism
  - Copyright issues îˆ€citeîˆ‚turn13view0îˆ  
- Potential misuse:
  - Fake news
  - Opinion manipulation îˆ€citeîˆ‚turn13view0îˆ  
- Requires:
  - Better data curation
  - Bias mitigation
  - Strong ethical guidelines îˆ€citeîˆ‚turn13view0îˆ  

## Multimodal and Generalist Models

### Multimodal models

Multimodal models are designed to process multiple types of data, moving beyond the traditional text based inputs. These models integrate different modalities such as text, image, audio, and video, allowing AI to understand and generate more contextually rich outputs. îˆ€citeîˆ‚turn3search0îˆ‚turn3search1îˆ  

Instead of relying on a single data type, multimodal models combine multiple inputs to enhance their decision making and interpretation. To achieve this, separate architectures are often used for different data types. îˆ€citeîˆ‚turn3search0îˆ‚turn3search1îˆ  

Examples from the notes:
- ViLBERT â€“ 2 separate models for text and videos îˆ€citeîˆ‚turn3search0îˆ  
- Show & Tell â€“ CNN based model for images, LSTM for text captioning îˆ€citeîˆ‚turn3search1îˆ  

Clarification (added to preserve intent while tightening accuracy):
- ViLBERT is a **two-stream** vision-and-language model with separate visual and textual streams that interact via co-attention; it is primarily presented for image+text settings but the same design pattern is often discussed in broader multimodal contexts. îˆ€citeîˆ‚turn3search0îˆ‚turn3search4îˆ  

Looking at the illustration (as described in the notes), different components work together:
- CNN extracts features from an image
- A separate model such as BiLSTM processes corresponding textual data
- Outputs are then pooled or chained to form a final structured response, ensuring both image and text contribute to overall understanding îˆ€citeîˆ‚turn3search1îˆ‚turn3search0îˆ  

This fusion enables tasks such as automatic captioning, video analysis, and even speech to text with contextual awareness. Multimodal learning enhances AI's ability to interpret the world more like humans by integrating multiple sensory inputs. îˆ€citeîˆ‚turn3search0îˆ‚turn3search1îˆ  

Applications mentioned:
- Autonomous systems
- Accessibility tools
- Interactive AI assistance îˆ€citeîˆ‚turn3search0îˆ‚turn3search1îˆ  

### Gato

GATO, developed by DeepMind in 2022, is a generalist deep neural network capable of handling text, images, video, and robotic control within a single Transformer architecture. îˆ€citeîˆ‚turn2search3îˆ‚turn2search7îˆ  

Unlike traditional multimodal models, GATO does not use separate CNNs or LSTMs; instead, it tokenises all inputs into a shared format, treating different modalities as a sequence. îˆ€citeîˆ‚turn2search3îˆ‚turn2search7îˆ  

This unified approach allows the model to handle diverse tasks without needing specialised architecture for each modality. îˆ€citeîˆ‚turn2search3îˆ‚turn2search7îˆ  

GATO has been trained across a wide range of applications, from chat bots and gaming to robotic control, demonstrating its adaptability. îˆ€citeîˆ‚turn2search3îˆ‚turn2search7îˆ  

Its versatility represents a major shift from specialised AI systems towards scalable generalist AI models that can efficiently operate across multiple domains. îˆ€citeîˆ‚turn2search3îˆ‚turn2search7îˆ  


## Useful Links
- https://paperswithcode.com/method/gptLinks to an external site.
- https://30dayscoding.com/blog/understanding-the-architecture-of-gpt-modelsLinks to an external site.
- https://arxiv.org/abs/1810.04805Links to an external site.
- https://huggingface.co/docs/transformers/en/model_doc/distilbertLinks to an external site.
- https://github.com/huggingface/transformersLinks to an external site.
- https://huggingface.co/docs/transformers/en/model_doc/bart

### Visualise Deep Learning Models

```text
https://projector.tensorflow.org/
https://adamharley.com/nn_vis/cnn/3d.html
```
