# Generative AI & Deep Learning Notes
### My Learning Notes from [NUS generative-ai-fundamentals-to-advanced-techniques-programme](https://nus.comp.emeritus.org/generative-ai-fundamentals-to-advanced-techniques-programme)

## Table of Contents
- [Learning Paradigms in Machine Learning](#learning-paradigms-in-machine-learning)
- [From Brains to Artificial Neural Networks](#from-brains-to-artificial-neural-networks)
- [Convolutional Neural Networks](#convolutional-neural-networks)
- [Transformers and Attention Mechanisms](#transformers-and-attention-mechanisms)
- [Transformer Model Families](#transformer-model-families)
- [Alignment, Reliability, and Knowledge Grounding](#alignment-reliability-and-knowledge-grounding)
- [Multimodal and Generalist Models](#multimodal-and-generalist-models)
- [Useful Links](#useful-links)



## Learning Paradigms in Machine Learning

### Reinforcement Learning vs Supervised Learning

- **Supervised Learning:** A learning paradigm where a model is trained on labelled input-output pairs and receives direct feedback on errors. citeîˆ‚turn20search0  
- **Reinforcement Learning (RL):** A learning paradigm where an agent learns by interacting with an environment using rewards and penalties. citeîˆ‚turn5search0  

#### Supervised Learning
1. Collect labelled data. citeîˆ‚turn20search0Ë™  
2. Split into training and validation sets. citeîˆ‚turn20search0  
3. Train model to minimize loss. citeîˆ‚turn20search0  
4. Evaluate performance. citeîˆ‚turn20search0  

#### Reinforcement Learning
1. Initialize agent and environment. citeîˆ‚turn5search0  
2. Perform actions based on current policy. citeîˆ‚turn5search0  
3. Receive reward feedback. citeîˆ‚turn5search0  
4. Update policy using reward signal. citeîˆ‚turn5search0  
5. Repeat until convergence. citeîˆ‚turn5search0  

#### Examples
- **Supervised Learning:** Image classification, spam detection, price prediction. citeîˆ‚turn20search0  
- **Reinforcement Learning:** Game playing, robotics control, recommendation strategies with delayed rewards. citeîˆ‚turn5search0îˆ‚turn2search3  

### Unsupervised Learning

- **Unsupervised Learning:** A branch of machine learning where models learn patterns and relationships from unlabelled data without predefined outputs. citeîˆ‚turn20search1  

#### Examples

##### Clustering Problems
- Customer segmentation citeîˆ‚turn20search1  
- Image segmentation citeîˆ‚turn20search1  

##### Dimensionality Reduction Problems
- Principal Component Analysis (PCA) citeîˆ‚turn4search12  
- t-SNE citeîˆ‚turn5search1  

##### Anomaly Detection
- Fraud detection citeîˆ‚turn20search7  

##### Generative Models (partly unsupervised)
- Autoencoders citeîˆ‚turn5search3  
- GANs (Generative Adversarial Networks) citeîˆ‚turn5search2  

##### Market Basket Analysis
- Identifying frequently co-occurring items citeîˆ‚turn20search6îˆ‚turn20search22  

## From Brains to Artificial Neural Networks

### The Human Brain and Neural Complexity

#### Overview
The human brain is an incredibly complex organ and one of natureâ€™s greatest engineering marvels.  
The **neocortex** plays a key role in higher cognitive functions such as:
- ğŸ§  Reasoning  
- ğŸ‘ï¸ Perception  
- ğŸ¤” Decision-making  
- ğŸ—£ï¸ Language  
Its massive connectivity gives the brain extraordinary computational power. citeîˆ‚turn4search5îˆ‚turn7search6  

#### Neurons and Synapses
- The brain contains around **100 billion neurons** ğŸ§© citeîˆ‚turn4search5  
- Each neuron connects to **1,000â€“10,000** other neurons through **synapses** citeîˆ‚turn7search6îˆ‚turn7search2  
- Synapses transmit information via âš¡ electrical and ğŸ§ª chemical signals citeîˆ‚turn7search2  
- The neocortex alone is estimated to have about **500 trillion synapses**, forming a massive biological network capable of learning and thought citeîˆ‚turn7search2îˆ‚turn8search0  

> Note (clarification added): Published estimates vary by method and definition; widely cited totals include ~86 billion neurons for the whole brain and on the order of 10^14 synapses overall, with neocortex synapse counts reported in the ~10^14 range as well. citeîˆ‚turn4search5îˆ‚turn7search2  

#### Neural Wiring and Efficiency
- The neocortex contains roughly **300 million feet (â‰ˆ91,440 km)** of neural wiring citeîˆ‚turn8search0  
- This wiring is compacted into a volume of about **1.5 quarts (â‰ˆ1.4 liters)** ğŸ¤¯ citeîˆ‚turn8search0  
- Such efficiency is achieved through several biological optimizations: citeîˆ‚turn8search0îˆ‚turn4search5  

##### Folding of the Cortex
- The brain surface is folded into **gyri** (ridges) and **sulci** (grooves)  
- Folding increases surface area without increasing overall volume citeîˆ‚turn8search0  

##### Myelination
- Axons are coated with **myelin**, a fatty insulating layer  
- Myelin speeds up signal transmission and reduces energy usage citeîˆ‚turn4search15  

##### Specialized Networks
- The brain is organized into specialized functional areas, such as:
  - ğŸ‘€ Visual cortex
  - âœ‹ Motor cortex
  - ğŸ§  Prefrontal cortex
- Specialization minimizes unnecessary wiring and improves processing speed citeîˆ‚turn8search0îˆ‚turn4search5  

### Neural Networks and Artificial Neural Networks

- Neural networks are inspired by biological brains. citeîˆ‚turn5search0  
- Artificial neurons approximate real neurons. citeîˆ‚turn5search0  
- ANNs are networks of artificial neurons. citeîˆ‚turn5search0  
- ANNs are simplified models of brain functionality. citeîˆ‚turn5search0  
- Practically, ANNs are parallel computational systems. citeîˆ‚turn5search0  

#### Definitions
- **Neural Networks (NNs):** Networks of neurons similar to those found in biological brains. citeîˆ‚turn5search0  
- **Artificial Neurons:** Crude approximations of biological neurons, implemented as mathematical or software constructs. citeîˆ‚turn5search0  
- **Artificial Neural Networks (ANNs):** Networks of artificial neurons that approximate certain functions of real brains. citeîˆ‚turn5search0  

### Biological vs Artificial Neurons

#### Biological Neurons
- Biological neurons have **synaptic gaps** of varying strengths ğŸ”— citeîˆ‚turn7search2  
- These synapses connect to the **soma (cell body)** ğŸ§  citeîˆ‚turn7search2  
- Signal strength depends on synaptic weight and connectivity citeîˆ‚turn7search2  
- Information flows via:
  - ğŸŒ¿ Dendrites (input)
  - ğŸ§  Cell body (integration)
  - âš¡ Axon (signal transmission)
  - ğŸ”š Axon terminals (output) citeîˆ‚turn7search2  

#### Artificial Neurons
- Artificial neurons replace synapses with **numerical inputs** citeîˆ‚turn5search0  
- Inputs can come from:
  - Other neurons
  - Sensors
  - Data features
  - Variables citeîˆ‚turn5search0  
- Core operations include:
  - â• Weighted sum (Î£)
  - ğŸ“‰ Activation / threshold function citeîˆ‚turn5search0  

#### Computational Power
- The neocortex contains about **500 trillion synapses** operating **in parallel** citeîˆ‚turn8search0îˆ‚turn7search2  
- Enables massive information processing and storage simultaneously citeîˆ‚turn7search2  
- The human brain operates on roughly **20 watts of power** ğŸ’¡ citeîˆ‚turn7search1îˆ‚turn4search8  
- This is far more **energy-efficient** than modern supercomputers citeîˆ‚turn7search1  

#### Implications for Intelligence and Learning
- High neuron density and interconnectivity enable human intelligence citeîˆ‚turn4search5  
- **Plasticity** allows neural connections to reorganize with:
  - ğŸ“š Learning
  - ğŸ§  Experience
  - ğŸ©¹ Recovery from injury citeîˆ‚turn4search5  
- This adaptability is central to skill acquisition and cognition citeîˆ‚turn4search5  

#### Artificial Neural Networks (ANNs)
- ANNs are inspired by biological neural systems citeîˆ‚turn5search0  
- They aim to approximate learning and decision-making citeîˆ‚turn5search0  
- While powerful, they are far less energy-efficient than the human brain citeîˆ‚turn7search1  
- The neocortex remains a benchmark for efficient computation and learning citeîˆ‚turn7search1îˆ‚turn4search5  

### Artificial Neural Networks. Why?

- ğŸ§® **Extremely powerful computational devices**  
  - Turing-equivalent universal computers citeîˆ‚turn5search0  
- âš¡ **Massive parallelism**  
  - Many simple units operate simultaneously, making computation efficient citeîˆ‚turn5search0  
- ğŸ“š **Learning and generalization**  
  - Learn directly from training data  
  - No need for carefully handcrafted rules or designs citeîˆ‚turn20search0  
- ğŸ›¡ï¸ **Fault-tolerant and noise-tolerant**  
  - Performance degrades gracefully even with imperfect data or failures citeîˆ‚turn5search0  
- ğŸ§  **Beyond symbolic systems**  
  - Can do everything a symbolic or logic-based system can, and more citeîˆ‚turn5search0  
- ğŸ“Š **Excellent with unstructured data**  
  - Particularly strong with:
    - ğŸ“ Text  
    - ğŸ–¼ï¸ Images  
    - ğŸ”Š Audio  
    - Other semi-structured data citeîˆ‚turn5search0  

## Convolutional Neural Networks

### Deep Convolutional Neural Networks (CNNs)

#### Definition
**Deep Convolutional Neural Networks (CNNs)** are a specialized type of neural network designed to process **structured, grid-like data**, especially **images** ğŸ–¼ï¸. citeîˆ‚turn4search1  

#### Key characteristics
- Designed for **spatial data** arranged in grids citeîˆ‚turn4search1  
- Combine:
  - ğŸ§  **Convolutional layers** for feature extraction
  - ğŸ”— **Fully connected layers** for decision-making citeîˆ‚turn4search1  
- Inspired by the **human visual system** citeîˆ‚turn4search1  
- Highly effective when **spatial hierarchies** matter citeîˆ‚turn4search1  

#### Core building blocks
- ğŸŸ¨ **Input**  
- ğŸŸ© **Convolutional Layer**. Extracts local features (edges, textures)
- ğŸŸ¦ **Pooling Layer**. Reduces spatial size and computation
- ğŸ§ª **Activation Function**. Adds non-linearity
- ğŸ”µ **Fully Connected Layer**. Performs classification or prediction
- ğŸ¯ **Output** citeîˆ‚turn4search1  

#### Why CNNs work well
- Learn **hierarchical features**. from simple edges to complex shapes citeîˆ‚turn4search1  
- Preserve **spatial relationships** in data citeîˆ‚turn4search1  
- Particularly strong for **vision-based tasks** citeîˆ‚turn4search1  

#### Typical use cases
- Image classification
- Object detection
- Image segmentation
- Visual pattern recognition citeîˆ‚turn4search1  

#### Advantages of CNNs
- âš™ï¸ **Automatic Feature Extraction**  
  - CNNs learn features directly from raw data  
  - No manual feature engineering required citeîˆ‚turn4search1  
- ğŸ” **Parameter Sharing**  
  - Same filters are reused across the image  
  - Fewer parameters than fully connected networks citeîˆ‚turn4search1  
- ğŸ“ **Translation Invariance**  
  - Recognize patterns (edges, shapes, objects) regardless of position in the image citeîˆ‚turn4search1  
- ğŸ“ **Efficient for High-Dimensional Data**  
  - Scales well to large images and datasets citeîˆ‚turn4search1  
- ğŸ† **State-of-the-Art Performance**  
  - Top results in:
    - Image classification
    - Object detection
    - Image segmentation citeîˆ‚turn4search1  
- ğŸŒ **Adaptability to Diverse Domains**  
  - Can be applied to:
    - ğŸ–¼ï¸ Images
    - ğŸ”Š Audio spectrograms
    - â±ï¸ Time-series data  
  - Requires minimal architectural changes citeîˆ‚turn4search1  

#### Limitations of CNNs
- ğŸ’» **Computationally Intensive**  
  - Training requires powerful hardware (GPUs, TPUs) citeîˆ‚turn4search1  
- ğŸ“Š **Data Hungry**  
  - Needs large labelled datasets for good performance  
  - Data collection and annotation can be expensive citeîˆ‚turn4search1  
- ğŸ” **Lack of Interpretability**  
  - Acts as a black-box model  
  - Difficult to understand or debug decisions citeîˆ‚turn4search1  
- ğŸ“‰ **Overfitting Risk**  
  - Without proper regularisation, models may memorise training data citeîˆ‚turn4search1  
- ğŸ›ï¸ **Sensitivity to Hyperparameters**  
  - Performance depends heavily on:
    - Architecture
    - Learning rate
    - Other tuning choices citeîˆ‚turn4search1  

## Transformers and Attention Mechanisms

### Attention Mechanism in Transformers

#### Attention
At the core of modern NLP lies **attention**.  
It allows models to **focus**, not memorise.  
Instead of treating all words equally, attention helps the model decide **what matters most** in context. citeîˆ‚turn15view0  

A commonly used formulation is scaled dot-product attention:  
**Attention(Q, K, V) = softmax(QKáµ€ / âˆšdâ‚–) Â· V**. citeîˆ‚turn15view0  

#### Query, Key, and Value (QKV)

To understand attention, everything revolves around three components:

- ğŸ” **Query (Q)**  
  - Represents the **current word or token**
  - Think of it as a word *asking* for relevant context citeîˆ‚turn15view0  

- ğŸ—ï¸ **Key (K)**  
  - Represents **all words in the input sequence**
  - Each word has a key used to measure relevance to the query citeîˆ‚turn15view0  

- ğŸ“¦ **Value (V)**  
  - Contains the **actual information** (embeddings)
  - This is the content passed forward once relevance is determined citeîˆ‚turn15view0  

#### How Attention Works (Step-by-Step)

- Compute **attention scores** using dot products between Query and Keys. citeîˆ‚turn15view0  
- Scale scores by **âˆšdâ‚–** for stability. citeîˆ‚turn15view0  
- Apply **softmax** to get a probability distribution over tokens. citeîˆ‚turn15view0  
- Take the **weighted sum of Values** to produce a context-aware representation. citeîˆ‚turn15view0  

#### Why Attention Matters
- Focuses on **relevant words**, regardless of position citeîˆ‚turn15view0  
- Handles **long-range dependencies** citeîˆ‚turn15view0  
- Essential for understanding meaning in complex sentences citeîˆ‚turn15view0  

### Multi-Head Attention

#### What is Multi-Head Attention?
- Instead of one attention mechanism, the model uses **multiple heads**
- Each head attends to **different aspects** of the sequence citeîˆ‚turn15view0  

#### How It Works
- Input is split into multiple Q, K, V sets  
- Each head computes attention **independently**  
- Outputs are:
  - Concatenated
  - Passed through a linear layer  
â¡ï¸ Result. richer and more expressive representations citeîˆ‚turn15view0  

### Feedforward Networks (FFN)

#### Role of Feedforward Networks
- Applied **after multi-head attention**
- Operates **independently on each position** citeîˆ‚turn15view0  

#### Structure
- Linear layer
- ReLU activation
- Linear layer citeîˆ‚turn15view0  

#### Purpose
- Captures **more abstract patterns**
- Refines representations beyond word-to-word relationships
- Output is passed to the next transformer layer citeîˆ‚turn15view0  

### Transformer Architecture. Big Picture

- ğŸ”¡ Embeddings citeîˆ‚turn15view0  
- ğŸ” Self-attention (QKV) citeîˆ‚turn15view0  
- ğŸ”€ Multi-head attention citeîˆ‚turn15view0  
- ğŸ§ª Feedforward networks citeîˆ‚turn15view0  
- ğŸ”„ Stacked layers for depth citeîˆ‚turn15view0  

Together, these components allow transformers to:
- Understand context deeply
- Scale efficiently
- Generate high-quality, human-like text citeîˆ‚turn15view0  

### Training a Transformer
- ğŸ”¡ Data preprocessing. tokenisation and embeddings citeîˆ‚turn15view0îˆ‚turn2search4  
- ğŸ“ Positional encodings added for sequence order citeîˆ‚turn15view0  
- ğŸ² Random weight initialisation citeîˆ‚turn15view0  
- ğŸ” Training via **backpropagation** citeîˆ‚turn15view0  
- ğŸ›ï¸ Hyperparameters tuned:
  - Learning rate
  - Batch size citeîˆ‚turn15view0îˆ‚turn2search4  
- Goal. minimise loss and optimise performance citeîˆ‚turn15view0  

### Computational Challenges
- Large models are **computationally intensive** citeîˆ‚turn15view0îˆ‚turn1search7  
- Example:
  - GPT-3 has **175 billion parameters** citeîˆ‚turn1search7  
- CPUs are insufficient for training
- Requires:
  - âš¡ GPUs
  - ğŸš€ TPUs citeîˆ‚turn1search7îˆ‚turn17view0  
- Computational needs have grown **exponentially**
- Hardware advances are critical for progress citeîˆ‚turn12search7  

### Transformer Variants
- Transformers excel at sequential data
- Variants optimise performance for specific tasks citeîˆ‚turn3search10îˆ‚turn4search0  

#### Hybrid Architectures

##### CNN + Transformer
- Common in computer vision
- CNN captures **local spatial features**
- Transformer captures **long-range dependencies** citeîˆ‚turn4search1îˆ‚turn15view0  
- Used in:
  - Vision Transformers (ViTs)
  - ResNet hybrids citeîˆ‚turn4search1  

##### RNN + Transformer
- Useful for:
  - ğŸ—£ï¸ Speech recognition
  - â±ï¸ Time-series forecasting citeîˆ‚turn15view0  
- RNN handles local sequences
- Transformer handles global context citeîˆ‚turn15view0  

##### Benefits
- Reduced computational cost
- Better efficiency and scalability
- Strong performance on complex tasks citeîˆ‚turn3search10îˆ‚turn4search0  

#### Efficient Transformers (Long Sequences)
- Standard transformers scale **quadratically** with sequence length due to self-attention citeîˆ‚turn3search10îˆ‚turn3search3îˆ‚turn4search0  
- Problematic for long documents
- Sparse and efficient variants address this citeîˆ‚turn3search10îˆ‚turn4search0  

##### Longformer
- Uses attention patterns combining local + global information
- Scales more linearly for long documents citeîˆ‚turn3search10îˆ‚turn3search2  
- Ideal for:
  - Document-level QA
  - Summarisation citeîˆ‚turn3search10  

##### Linformer
- Approximates attention via low-rank structure
- Reduces memory and inference cost for long sequences citeîˆ‚turn3search3  

##### Reformer
- Optimised for memory efficiency
- Uses locality-sensitive hashing (LSH) attention and reversible layers citeîˆ‚turn4search0  

## Transformer Model Families

### BERT vs GPT vs T5

#### BERT
- **Bidirectional Encoder Representations from Transformers**
- Understands text in **both directions** citeîˆ‚turn17view0  
- Best suited for:
  - ğŸ˜Š Sentiment analysis
  - ğŸ·ï¸ Text classification
  - â“ Question answering citeîˆ‚turn17view0  

#### GPT
- **Generative Pre-trained Transformer**
- **Autoregressive**. predicts the next word in a sequence citeîˆ‚turn15view0îˆ‚turn1search7  
- Ideal for:
  - ğŸ’¬ Conversational AI
  - âœï¸ Content generation
  - ğŸ’» Code generation citeîˆ‚turn1search7  

#### T5
- **Text-to-Text Transfer Transformer**
- Treats **all NLP tasks as text-to-text** citeîˆ‚turn2search4  
- Highly versatile for:
  - ğŸŒ Translation
  - ğŸ§¾ Summarisation
  - â“ Question answering citeîˆ‚turn2search4  

### BERT limitations and common variants

#### Limitations of BERT (encoder-only)
- **Context window limit:** BERT is commonly pre-trained with an input context window up to **512 tokens**. citeîˆ‚turn18view0îˆ‚turn0search13  
- **Dimensionality cost:** common configurations include hidden sizes like **768** (BERT-base), which affects memory and compute. citeîˆ‚turn0search13îˆ‚turn19search15  
- **No true free-form generation:** BERT is trained with a masked prediction objective (and NSP in the original version), so it excels at understanding/representation learning rather than left-to-right generation. citeîˆ‚turn17view0îˆ‚turn18view0  

#### Variants mentioned in the notes and discussion
- **RoBERTa:** improved BERT pretraining by changing key training choices (for example, removing NSP and using dynamic masking and more data/compute). citeîˆ‚turn0search6îˆ‚turn0search10  
- **ELECTRA:** replaces masked-token prediction with **replaced token detection** (discriminator predicts whether a token was replaced by a generator). citeîˆ‚turn0search3îˆ‚turn0search7  
- **DistilBERT:** uses knowledge distillation to create a smaller, faster model while preserving much of BERTâ€™s performance. citeîˆ‚turn11search0  
- **ALBERT:** reduces parameters (e.g., factorized embeddings and parameter sharing) to improve efficiency. citeîˆ‚turn11search5  
- **SpanBERT:** masks contiguous spans and trains objectives tailored to span representations. citeîˆ‚turn11search6  
- **CodeBERT:** pre-trained for natural language + programming language tasks (bimodal NLâ€“PL). citeîˆ‚turn11search3  

### GPT models and how ChatGPT works

#### Model Scale Comparison

| Model   | Number of Parameters |
|--------|----------------------|
| GPT-1  | 117M |
| GPT-2  | 1.5B |
| GPT-3  | 175B |
| GPT-3.5 | GPT-3 + ~6B |
| GPT-4  | ~1.7T |

Notes/clarifications added:
- GPT-2 (1.5B) and GPT-3 (175B) parameter counts are documented in their respective technical reports. citeîˆ‚turn1search0îˆ‚turn1search7  
- Some commonly repeated values for â€œGPT-3.5â€ and â€œGPT-4 parameter countâ€ are **not officially disclosed**. The GPT-4 technical report explicitly states it does not provide details such as model size. citeîˆ‚turn14view0  

#### How ChatGPT Works (End-to-End Flow)

â¡ï¸ **Pre-training**  
- Trained on massive internet text
- Learns to predict the **next token**
- Captures grammar, facts, and language patterns citeîˆ‚turn1search7îˆ‚turn13view0  

â¡ï¸ **Fine-tuning**  
- Uses datasets reviewed by **human trainers**
- Learns to generate safer and more helpful responses
- Generalises from human feedback citeîˆ‚turn1search1îˆ‚turn1search5  

â¡ï¸ **Input Processing**  
- User input is **tokenised** into words or subwords
- Tokens are fed into the transformer model citeîˆ‚turn2search4îˆ‚turn13view0  

â¡ï¸ **Contextual Understanding**  
- Maintains conversation history ğŸ§ 
- Transformer architecture models long-range dependencies
- Enables context-aware responses citeîˆ‚turn15view0îˆ‚turn13view0  

â¡ï¸ **Response Generation**  
- Predicts next tokens based on learned patterns
- Produces coherent, human-like text citeîˆ‚turn15view0îˆ‚turn13view0  

â¡ï¸ **Sampling and Optimisation**  
- Uses probabilistic sampling ğŸ²
- Adds controlled randomness for natural responses
- Safety techniques reduce harmful outputs citeîˆ‚turn6search0îˆ‚turn1search1îˆ‚turn13view0  

â¡ï¸ **Post-processing**  
- Removes special tokens and formatting
- Final response is shown to the user ğŸ’¬ citeîˆ‚turn13view0  

### Encoderâ€“decoder models and key examples

#### Encoder-decoder models
Encoderâ€“decoder models are a fundamental architecture in modern deep learning. These models bring together an encoder and a decoder, enabling efficient processing of input data while generating meaningful output. citeîˆ‚turn15view0îˆ‚turn2search4  

- Start with the encoder, which takes the input and processes it to capture its contextual meaning. It transforms the data into a structured representation that the model can understand. citeîˆ‚turn15view0  
- Then comes the decoder, which uses that structured information to generate text that is natural and coherent. Unlike simple text generation models that produce output sequentially, encoder-decoder models maintain logical consistency by referencing the complete context provided by the encoder. citeîˆ‚turn15view0  

#### Applications of encoderâ€“decoder models
Encoderâ€“decoder models power a variety of real-world applications including:
- Machine translation to convert text from one language to another citeîˆ‚turn15view0îˆ‚turn2search4  
- Text summarisation, extracting the key points while preserving the meaning citeîˆ‚turn2search4îˆ‚turn2search2  
- Caption generation, generating textual descriptions for images or videos citeîˆ‚turn3search1  

#### BART
BART (Bidirectional and Auto-Regressive Transformer) is a hybrid model that integrates the strengths of both BERT and GPT, making it highly effective for tasks requiring text reconstruction and controlled generation. citeîˆ‚turn2search1  

- It employs a bidirectional encoding process (similar to BERT) for comprehensive contextual understanding. citeîˆ‚turn2search1  
- It uses denoising objectives such as span corruption / text infilling (replacing spans with a single mask token), forcing reconstruction of phrases while maintaining coherence. citeîˆ‚turn2search1  
- On the decoding side, BART adopts autoregressive generation (similar to GPT), where tokens are generated one at a time while conditioning on the encoded input. citeîˆ‚turn2search1îˆ‚turn15view0  

##### BARTâ€™s Denoising Process
BART employs a denoising autoencoder approach, where input data is deliberately corrupted before being passed through the model. citeîˆ‚turn2search1  

Noise-insertion techniques described in the notes:
- Token masking replaces random words with a special [MASK] token. citeîˆ‚turn2search1  
- Token deletion removes entire words from the sequence. citeîˆ‚turn2search1  
- Text infilling (span corruption) replaces entire spans of text with a single [MASK] token. citeîˆ‚turn2search1  

This training strategy makes BART robust to noisy or incomplete inputs (e.g., imperfect formatting or missing spans) and supports tasks like summarisation, paraphrasing, translation, and text completion. citeîˆ‚turn2search1  

#### PEGASUS
PEGASUS (Pre-training with Extracted Gap-sentences for Abstractive Summarisation) is specifically designed to enhance text summarisation by masking entire sentences rather than individual tokens during pretraining. citeîˆ‚turn2search2îˆ‚turn2search10  

Key points from the notes:
- Entire sentences (instead of tokens) are masked during pretraining. citeîˆ‚turn2search2  
- Sentences may be randomly identified, though the method focuses on removing â€œimportantâ€ sentences (gap sentence generation). citeîˆ‚turn2search2  
- These are identified to be ones with high similarity to the rest of the document (intuitively encouraging summary-like targets). citeîˆ‚turn2search2  

PEGASUS achieves strong summarisation performance and can require minimal fine-tuning for high-quality abstractive summaries. citeîˆ‚turn2search2îˆ‚turn2search10  

#### T5
T5 (Text-to-Text Transfer Transformer) reformulates all NLP tasks into a text-to-text format, using an encoder-decoder architecture for tasks like translation, summarisation, and Q&A. citeîˆ‚turn2search4  

Text to Text
- Indicative of the types of input and output to be expected citeîˆ‚turn2search4  
- Encoder model used to get information for input text citeîˆ‚turn2search4  
- Decoder model used to generate output text citeîˆ‚turn2search4  

Transfer Transformer
- Transformer capable of employing transfer learning citeîˆ‚turn2search4  
- Allows for multiple NLP tasks to be accomplished by the model:
  - Translation
  - Summarisation
  - Q&A citeîˆ‚turn2search4  

T5 uses task prefixes (instructions) to unify workflows across tasks (e.g., â€œtranslate â€¦â€, â€œsummarize â€¦â€). citeîˆ‚turn2search4  

## Alignment, Reliability, and Knowledge Grounding

### GPT and Reinforcement Learning

GPT models, built on deep learning, have revolutionised language understanding and generation by predicting text patterns with remarkable fluency. Reinforcement Learning (RL), on the other hand, empowers systems to learn through trial and error, optimising actions for long-term rewards. Together, they unlock new frontiers in adaptive, intelligent decision-making and human-like interactions. citeîˆ‚turn5search0îˆ‚turn1search7  

### Reinforcement Learning from Human Feedback (RLHF)

Reinforcement Learning from Human Feedback (RLHF) enhances GPTâ€™s ability to generate not just human-like text but also reliable and contextually appropriate responses. citeîˆ‚turn1search1îˆ‚turn13view0  

Why RLHF is used (as captured in the notes and discussion):
- GPT can produce fluent language, but a pure Transformer architecture does not *inherently* verify factual accuracy or suitability of outputs. citeîˆ‚turn13view0îˆ‚turn1search1  
- Example (preserved): â€œUnder Augustus, the Roman Empire came to [MASK]â€ â€” GPT alone may not â€œknowâ€ which completion is historically correct without grounding or reliable internal knowledge. citeîˆ‚turn13view0îˆ‚turn1search1  

Typical RLHF-style pipeline:
- A model is fine-tuned using a **reward model** (often a transformer trained on human preference rankings) that prioritizes more useful outputs. citeîˆ‚turn1search1îˆ‚turn1search5  
- Human reviewers evaluate and correct a subset of responses, reinforcing high-quality and informative text generation. citeîˆ‚turn1search1îˆ‚turn1search5  

### Retrieval-Augmented Generation (RAG)

Retrieval-Augmented Generation (RAG) enhances GPT by integrating external knowledge sources, addressing its limitations with fixed training data. citeîˆ‚turn1search2îˆ‚turn1search6  

Key points (preserved and clarified):
- ChatGPT generates responses off fixed training data. citeîˆ‚turn13view0  
- Without external sources, it struggles with real-time updates, niche topics, or retrieving specific factual details. citeîˆ‚turn1search2îˆ‚turn13view0  
- RAG retrieves relevant documents or data (e.g., knowledge bases, web snapshots, or indexed corpora), feeding them into the model as additional context for response generation. citeîˆ‚turn1search2îˆ‚turn1search6  
- This approach improves accuracy, reduces hallucinations, and enables up-to-date, domain-specific answers. citeîˆ‚turn1search2îˆ‚turn13view0  
- Combining GPT-style models with retrieval creates more reliable, informed, and adaptable AI systems for business and research applications. citeîˆ‚turn1search2îˆ‚turn1search6  

### Zero-Shot Learning (ZSL)

Zero-Shot Learning (ZSL) enables GPT to perform tasks without additional training, relying solely on its extensive pretraining. Instead of requiring labeled examples or fine-tuning: citeîˆ‚turn1search7  

- GPT leverages its extensive pre-training to perform tasks without additional training, generating relevant outputs directly from prompt instruction. citeîˆ‚turn1search7  
- Enhanced flexibility: allows adaptation to new tasks without extra fine-tuning. citeîˆ‚turn1search7  
- Streamlined workflow: reduces the need for task-specific fine-tuning. citeîˆ‚turn1search7  
- Can be more efficient for real-time applicationsâ€”enhancing overall productivity. citeîˆ‚turn1search7  

### Model Temperature

Temperature is a parameter that controls the randomness of token selection during inference (commonly described as scaling logits before softmax). citeîˆ‚turn6search0  

Notes (preserved):
- Future tokens picked via probability distribution citeîˆ‚turn6search0  
- Higher probability = higher chance of selection citeîˆ‚turn6search0  
- Temperature scaling adjusts randomness in word selection. citeîˆ‚turn6search0  
- Tuning temperature balances precision and creativity in generated text. citeîˆ‚turn6search0  

### Challenges in Text Generation

#### Hallucinations
- Generates plausible but incorrect information
- Caused by missing or weak training signals citeîˆ‚turn13view0  

#### Bias
- Models inherit biases from training data
- Can reflect:
  - Gender bias
  - Cultural bias
  - Racial bias citeîˆ‚turn13view0  

#### Ethics
- Risks include:
  - Misinformation
  - Plagiarism
  - Copyright issues citeîˆ‚turn13view0  
- Potential misuse:
  - Fake news
  - Opinion manipulation citeîˆ‚turn13view0  
- Requires:
  - Better data curation
  - Bias mitigation
  - Strong ethical guidelines citeîˆ‚turn13view0  

## Multimodal and Generalist Models

### Multimodal models

Multimodal models are designed to process multiple types of data, moving beyond the traditional text based inputs. These models integrate different modalities such as text, image, audio, and video, allowing AI to understand and generate more contextually rich outputs. citeîˆ‚turn3search0îˆ‚turn3search1  

Instead of relying on a single data type, multimodal models combine multiple inputs to enhance their decision making and interpretation. To achieve this, separate architectures are often used for different data types. citeîˆ‚turn3search0îˆ‚turn3search1  

Examples from the notes:
- ViLBERT â€“ 2 separate models for text and videos citeîˆ‚turn3search0  
- Show & Tell â€“ CNN based model for images, LSTM for text captioning citeîˆ‚turn3search1  

Clarification (added to preserve intent while tightening accuracy):
- ViLBERT is a **two-stream** vision-and-language model with separate visual and textual streams that interact via co-attention; it is primarily presented for image+text settings but the same design pattern is often discussed in broader multimodal contexts. citeîˆ‚turn3search0îˆ‚turn3search4  

Looking at the illustration (as described in the notes), different components work together:
- CNN extracts features from an image
- A separate model such as BiLSTM processes corresponding textual data
- Outputs are then pooled or chained to form a final structured response, ensuring both image and text contribute to overall understanding citeîˆ‚turn3search1îˆ‚turn3search0  

This fusion enables tasks such as automatic captioning, video analysis, and even speech to text with contextual awareness. Multimodal learning enhances AI's ability to interpret the world more like humans by integrating multiple sensory inputs. citeîˆ‚turn3search0îˆ‚turn3search1  

Applications mentioned:
- Autonomous systems
- Accessibility tools
- Interactive AI assistance citeîˆ‚turn3search0îˆ‚turn3search1  

### Gato

GATO, developed by DeepMind in 2022, is a generalist deep neural network capable of handling text, images, video, and robotic control within a single Transformer architecture. citeîˆ‚turn2search3îˆ‚turn2search7  

Unlike traditional multimodal models, GATO does not use separate CNNs or LSTMs; instead, it tokenises all inputs into a shared format, treating different modalities as a sequence. citeîˆ‚turn2search3îˆ‚turn2search7  

This unified approach allows the model to handle diverse tasks without needing specialised architecture for each modality. citeîˆ‚turn2search3îˆ‚turn2search7  

GATO has been trained across a wide range of applications, from chat bots and gaming to robotic control, demonstrating its adaptability. citeîˆ‚turn2search3îˆ‚turn2search7  

Its versatility represents a major shift from specialised AI systems towards scalable generalist AI models that can efficiently operate across multiple domains. citeîˆ‚turn2search3îˆ‚turn2search7  


## Useful Links
- https://paperswithcode.com/method/gpt
- https://30dayscoding.com/blog/understanding-the-architecture-of-gpt-models
- https://arxiv.org/abs/1810.04805
- https://huggingface.co/docs/transformers/en/model_doc/distilbert
- https://github.com/huggingface/transformers
- https://huggingface.co/docs/transformers/en/model_doc/bart

### Visualise Deep Learning Models

```text
https://projector.tensorflow.org/
https://adamharley.com/nn_vis/cnn/3d.html
```
